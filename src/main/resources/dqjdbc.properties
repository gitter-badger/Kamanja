# zookeeper parameters
zookeeper.connect=127.0.0.1:2181
#zookeeper.session.timeout.ms=400
#zookeeper.sync.time.ms=200
 
# kafka parameters
kafka.group.id= dqjdbcsink
kafka.topic=cops_dq

# These parameters will be passed to kafka consumer config.  
# offset storage can be kafka or zookeeper. default is zookeeper
#kafka.offsets.storage=kafka
# behavior if consumer offsets for above group.id are present. 
# can be smallest meaning read from beginning or largest meaning read only new messages
#kafka.auto.offset.reset=smallest

# number of parallel kafka consumers to run
consumer.threads=1

# implementation class to process messages
adapter.message.processor=com.ligadata.adapters.jdbc.UpsertJDBCSink

# SimpleDateFormat format string used to parse date in input message
input.date.format=yyyy-MM-dd'T'HH:mm:ss.SSS

# jdbc connection
jdbc.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
jdbc.url=jdbc:sqlserver://ec2-54-160-245-36.compute-1.amazonaws.com:1433;databaseName=admin;
jdbc.user=admin
jdbc.password=pass123
jdbc.insert.statement=INSERT INTO [DAILY_DQ] ([APPID],[EVENT_DATE],[AVERAGE_DQ]) VALUES ({$appId},{$datetime},{$DailyAggs})
jdbc.update.statement=UPDATE [DAILY_DQ] SET [AVERAGE_DQ] = {$DailyAggs} WHERE [APPID] = {$appId} AND [EVENT_DATE] = {$datetime}


# parameters to control message batching
# messages will be written every "count" messages or every "interval" seconds
sync.messages.count=3
sync.interval.seconds=120
